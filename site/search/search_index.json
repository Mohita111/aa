{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"%23%20Cache-Augmented%20Generation/","text":"Augmented Reality (AR) in Python AR(Augmented Reality) is a system that combines the real and virtual worlds. Have you ever played \"Pokemon Go\"? If you are not aware of the game, the player has to turn on the camera and roam around places to find a 3D pokemon placed somewhere randomly in the virtual-real world on the console of the app. The player with the highest collection of pokemons wins. The concept that the player is able to see a virtual cartoon character in the real world in the game is AR(Augmented Reality). The game uses location tracking and geographical mapping and runs on AR technology. It creates a view of virtual objects in the real world . There is another technology called \"Virtual Reality\". AR and VR are not the same . Virtual reality is \"virtual world made real\" but Augmented reality is combining virtual objects with the real world. VR is about 75 percent virtual but AR is only 25 percent virtual . The difference between AR and VR can be well understood with examples. Playing games with VR headset makes the player involve in a virtual world while playing AR games involves virtual world into real world locations. There are two major goals for AR technology: Real-time interaction Accurate 3D representation of real and virtual objects. This tutorial gives a brief idea of AR using the OpenCV library in Python. Using the available functionality in Python, we'll try to implement some interesting AR ideas. How does a Computer Read an Image? A computer doesn't have the capability of recognizing images by its own. An image is a collection of pixels or picture elements. When we say \" My TV's resolution is 1080p \", we mean that the height*width ratio of the screen is 3840*2160 which means, the TV screen has over 8 million pixels. A pixel is like a small dot measuring about 0.26mm or 1/96 of an inch (varies). The computer can only recognize numbers. Hence, every pixel is represented using different color models. Most commonly used color models are RGB (Red green blue) and CMYK (Cyan Magenta Yellow Black) . RGB model: To represent a black and white image , each pixel is given a single number that represents the amount of light (contrast). The number ranges from 0 to 255. 0 represents no light ( black ) and 255 represents full light ( white ) and the numbers in the interval represent various shades of grey. To represent a color image in RGB model, every pixel is given 3 values each representing the amount of Red, green and blue colors in the pixel respectively. Processing Images with OpenCV library In the vast libraries of Python, OpenCV is one of the notable ones. It is a huge open source library that is used a lot for computer vision, machine learning applications . Using this library, we can process images and even videos. To process an image, every pixel is stored as a tuple of (B, G, R) values. There are various functions in OpenCV for processing images. Here are some basic functions: import cv2 pic = cv2.imread(r\"C:\\Users\\Jeevani\\Desktop\\6946b76fed14793869229898fc827e43.png\") Displaying the image print ('The picture we\\'re working on:') cv2.imshow('Multiverse spidey', pic) cv2.waitKey(0) Get image size and color model dim = pic.shape print ('\\nHeight: {}'.format(dim[0])) print ('Width: {}'.format(dim[1])) print ('Number of channels: {}'.format(dim[2])) BGR intensities in a pixel (blue, green, red) = pic[599, 499] print (\"\\nNo-of blue channels:\", blue) print (\"No-of green channels:\", green) print (\"No-of red channels:\", red) Focusing on a region in the picture print ('\\nSelecting a region in the image:') region = pic[200: 600, 300 : 600] cv2.imshow('Multiverse spidey', region) cv2.waitKey(0) Resizing the image print (\"\\nResizing the image to half by maintaining the aspect ratio:\") ratio = 50 h = int(dim[0]*ratio/100) w = int(dim[1]*ratio/100) size = (h,w) crop = cv2.resize(pic, size) cv2.imshow('Multiverse spidey', crop) cv2.waitKey(0) Rotating the image print (\"\\nRotating the image by 60 degree clockwise:\") h = int(dim[0]) w = int(dim[1]) center = (h//2, w//2) mat = cv2.getRotationMatrix2D(center, -60, 1) ro_pic = cv2.warpAffine(pic, mat, (h, w)) cv2.imshow('Multiverse spidey', ro_pic) cv2.waitKey(0) Output: The picture we're working on: Height: 621 Width: 500 Number of channels: 3 No-of blue channels: 12 No-of green channels: 12 No-of red channels: 12","title":"Augmented Reality (AR) in Python"},{"location":"%23%20Cache-Augmented%20Generation/#augmented-reality-ar-in-python","text":"AR(Augmented Reality) is a system that combines the real and virtual worlds. Have you ever played \"Pokemon Go\"? If you are not aware of the game, the player has to turn on the camera and roam around places to find a 3D pokemon placed somewhere randomly in the virtual-real world on the console of the app. The player with the highest collection of pokemons wins. The concept that the player is able to see a virtual cartoon character in the real world in the game is AR(Augmented Reality). The game uses location tracking and geographical mapping and runs on AR technology. It creates a view of virtual objects in the real world . There is another technology called \"Virtual Reality\". AR and VR are not the same . Virtual reality is \"virtual world made real\" but Augmented reality is combining virtual objects with the real world. VR is about 75 percent virtual but AR is only 25 percent virtual . The difference between AR and VR can be well understood with examples. Playing games with VR headset makes the player involve in a virtual world while playing AR games involves virtual world into real world locations. There are two major goals for AR technology: Real-time interaction Accurate 3D representation of real and virtual objects. This tutorial gives a brief idea of AR using the OpenCV library in Python. Using the available functionality in Python, we'll try to implement some interesting AR ideas.","title":"Augmented Reality (AR) in Python"},{"location":"%23%20Cache-Augmented%20Generation/#how-does-a-computer-read-an-image","text":"A computer doesn't have the capability of recognizing images by its own. An image is a collection of pixels or picture elements. When we say \" My TV's resolution is 1080p \", we mean that the height*width ratio of the screen is 3840*2160 which means, the TV screen has over 8 million pixels. A pixel is like a small dot measuring about 0.26mm or 1/96 of an inch (varies). The computer can only recognize numbers. Hence, every pixel is represented using different color models. Most commonly used color models are RGB (Red green blue) and CMYK (Cyan Magenta Yellow Black) .","title":"How does a Computer Read an Image?"},{"location":"%23%20Cache-Augmented%20Generation/#rgb-model","text":"To represent a black and white image , each pixel is given a single number that represents the amount of light (contrast). The number ranges from 0 to 255. 0 represents no light ( black ) and 255 represents full light ( white ) and the numbers in the interval represent various shades of grey. To represent a color image in RGB model, every pixel is given 3 values each representing the amount of Red, green and blue colors in the pixel respectively.","title":"RGB model:"},{"location":"%23%20Cache-Augmented%20Generation/#processing-images-with-opencv-library","text":"In the vast libraries of Python, OpenCV is one of the notable ones. It is a huge open source library that is used a lot for computer vision, machine learning applications . Using this library, we can process images and even videos. To process an image, every pixel is stored as a tuple of (B, G, R) values. There are various functions in OpenCV for processing images. Here are some basic functions: import cv2 pic = cv2.imread(r\"C:\\Users\\Jeevani\\Desktop\\6946b76fed14793869229898fc827e43.png\")","title":"Processing Images with OpenCV library"},{"location":"%23%20Cache-Augmented%20Generation/#displaying-the-image","text":"print ('The picture we\\'re working on:') cv2.imshow('Multiverse spidey', pic) cv2.waitKey(0)","title":"Displaying the image"},{"location":"%23%20Cache-Augmented%20Generation/#get-image-size-and-color-model","text":"dim = pic.shape print ('\\nHeight: {}'.format(dim[0])) print ('Width: {}'.format(dim[1])) print ('Number of channels: {}'.format(dim[2]))","title":"Get image size and color model"},{"location":"%23%20Cache-Augmented%20Generation/#bgr-intensities-in-a-pixel","text":"(blue, green, red) = pic[599, 499] print (\"\\nNo-of blue channels:\", blue) print (\"No-of green channels:\", green) print (\"No-of red channels:\", red)","title":"BGR intensities in a pixel"},{"location":"%23%20Cache-Augmented%20Generation/#focusing-on-a-region-in-the-picture","text":"print ('\\nSelecting a region in the image:') region = pic[200: 600, 300 : 600] cv2.imshow('Multiverse spidey', region) cv2.waitKey(0)","title":"Focusing on a region in the picture"},{"location":"%23%20Cache-Augmented%20Generation/#resizing-the-image","text":"print (\"\\nResizing the image to half by maintaining the aspect ratio:\") ratio = 50 h = int(dim[0]*ratio/100) w = int(dim[1]*ratio/100) size = (h,w) crop = cv2.resize(pic, size) cv2.imshow('Multiverse spidey', crop) cv2.waitKey(0)","title":"Resizing the image"},{"location":"%23%20Cache-Augmented%20Generation/#rotating-the-image","text":"print (\"\\nRotating the image by 60 degree clockwise:\") h = int(dim[0]) w = int(dim[1]) center = (h//2, w//2) mat = cv2.getRotationMatrix2D(center, -60, 1) ro_pic = cv2.warpAffine(pic, mat, (h, w)) cv2.imshow('Multiverse spidey', ro_pic) cv2.waitKey(0) Output: The picture we're working on: Height: 621 Width: 500 Number of channels: 3 No-of blue channels: 12 No-of green channels: 12 No-of red channels: 12","title":"Rotating the image"},{"location":"Augmented%20Reality/","text":"Augmented Reality (AR) in Python AR(Augmented Reality) is a system that combines the real and virtual worlds. Have you ever played \"Pokemon Go\"? If you are not aware of the game, the player has to turn on the camera and roam around places to find a 3D pokemon placed somewhere randomly in the virtual-real world on the console of the app. The player with the highest collection of pokemons wins. The concept that the player is able to see a virtual cartoon character in the real world in the game is AR(Augmented Reality). The game uses location tracking and geographical mapping and runs on AR technology. It creates a view of virtual objects in the real world . There is another technology called \"Virtual Reality\". AR and VR are not the same . Virtual reality is \"virtual world made real\" but Augmented reality is combining virtual objects with the real world. VR is about 75 percent virtual but AR is only 25 percent virtual . The difference between AR and VR can be well understood with examples. Playing games with VR headset makes the player involve in a virtual world while playing AR games involves virtual world into real world locations. There are two major goals for AR technology: Real-time interaction Accurate 3D representation of real and virtual objects. This tutorial gives a brief idea of AR using the OpenCV library in Python. Using the available functionality in Python, we'll try to implement some interesting AR ideas. How does a Computer Read an Image? A computer doesn't have the capability of recognizing images by its own. An image is a collection of pixels or picture elements. When we say \" My TV's resolution is 1080p \", we mean that the height*width ratio of the screen is 3840*2160 which means, the TV screen has over 8 million pixels. A pixel is like a small dot measuring about 0.26mm or 1/96 of an inch (varies). The computer can only recognize numbers. Hence, every pixel is represented using different color models. Most commonly used color models are RGB (Red green blue) and CMYK (Cyan Magenta Yellow Black) . RGB model: To represent a black and white image , each pixel is given a single number that represents the amount of light (contrast). The number ranges from 0 to 255. 0 represents no light ( black ) and 255 represents full light ( white ) and the numbers in the interval represent various shades of grey. To represent a color image in RGB model, every pixel is given 3 values each representing the amount of Red, green and blue colors in the pixel respectively. Processing Images with OpenCV library In the vast libraries of Python, OpenCV is one of the notable ones. It is a huge open source library that is used a lot for computer vision, machine learning applications . Using this library, we can process images and even videos. To process an image, every pixel is stored as a tuple of (B, G, R) values. There are various functions in OpenCV for processing images. Here are some basic functions: import cv2 pic = cv2.imread(r\"C:\\Users\\Jeevani\\Desktop\\6946b76fed14793869229898fc827e43.png\") Displaying the image print ('The picture we\\'re working on:') cv2.imshow('Multiverse spidey', pic) cv2.waitKey(0) Get image size and color model dim = pic.shape print ('\\nHeight: {}'.format(dim[0])) print ('Width: {}'.format(dim[1])) print ('Number of channels: {}'.format(dim[2])) BGR intensities in a pixel (blue, green, red) = pic[599, 499] print (\"\\nNo-of blue channels:\", blue) print (\"No-of green channels:\", green) print (\"No-of red channels:\", red) Focusing on a region in the picture print ('\\nSelecting a region in the image:') region = pic[200: 600, 300 : 600] cv2.imshow('Multiverse spidey', region) cv2.waitKey(0) Resizing the image print (\"\\nResizing the image to half by maintaining the aspect ratio:\") ratio = 50 h = int(dim[0]*ratio/100) w = int(dim[1]*ratio/100) size = (h,w) crop = cv2.resize(pic, size) cv2.imshow('Multiverse spidey', crop) cv2.waitKey(0) Rotating the image print (\"\\nRotating the image by 60 degree clockwise:\") h = int(dim[0]) w = int(dim[1]) center = (h//2, w//2) mat = cv2.getRotationMatrix2D(center, -60, 1) ro_pic = cv2.warpAffine(pic, mat, (h, w)) cv2.imshow('Multiverse spidey', ro_pic) cv2.waitKey(0) Output: The picture we're working on: Height: 621 Width: 500 Number of channels: 3 No-of blue channels: 12 No-of green channels: 12 No-of red channels: 12","title":"Augmented Reality (AR) in Python"},{"location":"Augmented%20Reality/#augmented-reality-ar-in-python","text":"AR(Augmented Reality) is a system that combines the real and virtual worlds. Have you ever played \"Pokemon Go\"? If you are not aware of the game, the player has to turn on the camera and roam around places to find a 3D pokemon placed somewhere randomly in the virtual-real world on the console of the app. The player with the highest collection of pokemons wins. The concept that the player is able to see a virtual cartoon character in the real world in the game is AR(Augmented Reality). The game uses location tracking and geographical mapping and runs on AR technology. It creates a view of virtual objects in the real world . There is another technology called \"Virtual Reality\". AR and VR are not the same . Virtual reality is \"virtual world made real\" but Augmented reality is combining virtual objects with the real world. VR is about 75 percent virtual but AR is only 25 percent virtual . The difference between AR and VR can be well understood with examples. Playing games with VR headset makes the player involve in a virtual world while playing AR games involves virtual world into real world locations. There are two major goals for AR technology: Real-time interaction Accurate 3D representation of real and virtual objects. This tutorial gives a brief idea of AR using the OpenCV library in Python. Using the available functionality in Python, we'll try to implement some interesting AR ideas.","title":"Augmented Reality (AR) in Python"},{"location":"Augmented%20Reality/#how-does-a-computer-read-an-image","text":"A computer doesn't have the capability of recognizing images by its own. An image is a collection of pixels or picture elements. When we say \" My TV's resolution is 1080p \", we mean that the height*width ratio of the screen is 3840*2160 which means, the TV screen has over 8 million pixels. A pixel is like a small dot measuring about 0.26mm or 1/96 of an inch (varies). The computer can only recognize numbers. Hence, every pixel is represented using different color models. Most commonly used color models are RGB (Red green blue) and CMYK (Cyan Magenta Yellow Black) .","title":"How does a Computer Read an Image?"},{"location":"Augmented%20Reality/#rgb-model","text":"To represent a black and white image , each pixel is given a single number that represents the amount of light (contrast). The number ranges from 0 to 255. 0 represents no light ( black ) and 255 represents full light ( white ) and the numbers in the interval represent various shades of grey. To represent a color image in RGB model, every pixel is given 3 values each representing the amount of Red, green and blue colors in the pixel respectively.","title":"RGB model:"},{"location":"Augmented%20Reality/#processing-images-with-opencv-library","text":"In the vast libraries of Python, OpenCV is one of the notable ones. It is a huge open source library that is used a lot for computer vision, machine learning applications . Using this library, we can process images and even videos. To process an image, every pixel is stored as a tuple of (B, G, R) values. There are various functions in OpenCV for processing images. Here are some basic functions: import cv2 pic = cv2.imread(r\"C:\\Users\\Jeevani\\Desktop\\6946b76fed14793869229898fc827e43.png\")","title":"Processing Images with OpenCV library"},{"location":"Augmented%20Reality/#displaying-the-image","text":"print ('The picture we\\'re working on:') cv2.imshow('Multiverse spidey', pic) cv2.waitKey(0)","title":"Displaying the image"},{"location":"Augmented%20Reality/#get-image-size-and-color-model","text":"dim = pic.shape print ('\\nHeight: {}'.format(dim[0])) print ('Width: {}'.format(dim[1])) print ('Number of channels: {}'.format(dim[2]))","title":"Get image size and color model"},{"location":"Augmented%20Reality/#bgr-intensities-in-a-pixel","text":"(blue, green, red) = pic[599, 499] print (\"\\nNo-of blue channels:\", blue) print (\"No-of green channels:\", green) print (\"No-of red channels:\", red)","title":"BGR intensities in a pixel"},{"location":"Augmented%20Reality/#focusing-on-a-region-in-the-picture","text":"print ('\\nSelecting a region in the image:') region = pic[200: 600, 300 : 600] cv2.imshow('Multiverse spidey', region) cv2.waitKey(0)","title":"Focusing on a region in the picture"},{"location":"Augmented%20Reality/#resizing-the-image","text":"print (\"\\nResizing the image to half by maintaining the aspect ratio:\") ratio = 50 h = int(dim[0]*ratio/100) w = int(dim[1]*ratio/100) size = (h,w) crop = cv2.resize(pic, size) cv2.imshow('Multiverse spidey', crop) cv2.waitKey(0)","title":"Resizing the image"},{"location":"Augmented%20Reality/#rotating-the-image","text":"print (\"\\nRotating the image by 60 degree clockwise:\") h = int(dim[0]) w = int(dim[1]) center = (h//2, w//2) mat = cv2.getRotationMatrix2D(center, -60, 1) ro_pic = cv2.warpAffine(pic, mat, (h, w)) cv2.imshow('Multiverse spidey', ro_pic) cv2.waitKey(0) Output: The picture we're working on: Height: 621 Width: 500 Number of channels: 3 No-of blue channels: 12 No-of green channels: 12 No-of red channels: 12","title":"Rotating the image"},{"location":"Cache-Augmented%20Generation/","text":"Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial Retrieval-augmented generation (RAG) is a powerful method to connect external knowledge bases to an LLM and fetch context each time a user asks a question, but it can slow down the LLM\u2019s performance due to its retrieval latency. Cache-augmented generation (CAG) offers a faster alternative; instead of performing real-time retrieval, it preloads your relevant documents into the model\u2019s context and stores that inference state \u2014 also known as a Key-Value (KV) cache. This approach eliminates retrieval latencies, allowing the model to access preloaded information instantly for faster and more efficient responses. For a more technical explanation of CAG, check out this article . In this tutorial, we will show how to build a simple CAG setup to embed all your knowledge upfront, quickly answer multiple user queries, and reset the cache without reloading the entire context each time. Prerequisites 1. A HuggingFace account and a HuggingFace access token 2. A document.txt file with sentences about yourself. Project Setup We import the essential libraries: torch for PyTorch. transformers for Hugging Face. DynamicCache for storing the model\u2019s key-value states. import torch from transformers import AutoTokenizer, AutoModelForCausalLM from transformers.cache_utils import DynamicCache import os Generate Function We\u2019ll next define the generate function. The generate function handles token-by-token generation with the cached knowledge using greedy decoding. Greedy decoding is a simple text generation method where, at each step, the token with the highest probability (maximum value in the logits) is selected as the next token. We pass in these inputs: model : The LLM, which with me Mistral-7B for this tutorial. input_ids : A tensor containing the tokenized input sequence. past_key_values : The core component of the CAG. A cache of previously computed attention values is used to speed up inference by avoiding recomputation. max_new_tokens : The maximum number of new tokens to generate. The default is 50. The function operates in a loop that iterates up to max_new_tokens times or terminates early if an end-of-sequence token (if configured) is generated. At each iteration: The model processes the current input tokens along with the cached past_key_values , producing logits for the next token. The logits are analyzed to identify the token with the highest probability using greedy decoding. This new token is appended to the output sequence, and the cache ( past_key_values ) is updated to include the current context. The newly generated token becomes the input for the next iteration. def generate(model, input_ids: torch.Tensor, past_key_values, max_new_tokens: int = 50) -> torch.Tensor: device = model.model.embed_tokens.weight.device origin_len = input_ids.shape[-1] input_ids = input_ids.to(device) output_ids = input_ids.clone() next_token = input_ids with torch.no\\_grad(): for \\_ in range(max\\_new\\_tokens): out = model( input\\_ids=next\\_token, past\\_key\\_values=past\\_key\\_values, use\\_cache=True ) logits = out.logits\\[:, -1, :\\] token = torch.argmax(logits, dim=-1, keepdim=True) output\\_ids = torch.cat(\\[output\\_ids, token\\], dim=-1) past\\_key\\_values = out.past\\_key\\_values next\\_token = token.to(device) if model.config.eos\\_token\\_id is not None and token.item() == model.config.eos\\_token\\_id: break return output\\_ids\\[:, origin\\_len:\\] DynamicCache Setup Next, we\u2019ll define the get_kv_cache function that prepares a reusable key-value cache for a transformer model\u2019s attention mechanism and the clean_up function that cleans the key-value cache by removing unnecessary entries to ensure that you can answer multiple independent questions without \u201cpolluting\u201d the cache. get_kv_cache passes a prompt (in our case, the knowledge from document.txt ) through the model once, creating a KV cache that records all the hidden states from each layer. get_kv_cache passes in these inputs: model : The transformer model used for encoding the prompt. tokenizer : Tokenizer to convert the prompt into token IDs. prompt : A string input is used as the prompt. and returns an object of the type DynamicCache. The get_kv_cache function first tokenizes the provided prompt using the tokenizer, converts it into input IDs, and then initializes an DynamicCache object to store key-value pairs, and then performs a forward pass through the model with caching enabled ( use_cache=True ). This populates the cache with the key-value pairs resulting from the model's computation. The clean_up trims a DynamicCache object to match the original sequence length by removing any additional tokens added during processing. For each layer of the cache, it slices both the key and value tensors to retain only the first origin_len tokens along the sequence dimension. def get_kv_cache(model, tokenizer, prompt: str) -> DynamicCache: device = model.model.embed_tokens.weight.device input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) cache = DynamicCache() with torch.no\\_grad(): \\_ = model( input\\_ids=input\\_ids, past\\_key\\_values=cache, use\\_cache=True ) return cache def clean_up(cache: DynamicCache, origin_len: int): for i in range(len(cache.key_cache)): cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :] cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :] Load LLM (Mistral) Now we\u2019ll load the Mistral-7B model, and load the tokenizer and model in full precision or half precision (FP16) on GPU if available. Remember to input YOUR_HF_TOKEN with your unique HuggingFace Token. model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"YOUR_HF_TOKEN\", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map=\"auto\", trust_remote_code=True, token=\"YOUR_HF_TOKEN\" ) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model.to(device) print(f\"Loaded {model_name}.\") Create a Knowledge Prompt from document.txt Next, we\u2019ll read document.txt , which you can fill with information about yourself. For this tutorial, document.txt contains information about me (Ronan Takizawa). Here we construct a simple system prompt embedding with the doc information and pass it to get_kv_cache to generate the KV cache. with open(\"document.txt\", \"r\", encoding=\"utf-8\") as f: doc_text = f.read() system_prompt = f\"\"\" <|system|> You are an assistant who provides concise factual answers. <|user|> Context: {doc_text} Question: \"\"\".strip() ronan_cache = get_kv_cache(model, tokenizer, system_prompt) origin_len = ronan_cache.key_cache[0].shape[-2] print(\"KV cache built.\") Ask Questions Reusing the Cache We first run clean_up to clear our cache (Good practice for CAGs). Next, we convert our questions into tokens in input_ids_q1 , then appended to the knowledge context stored in ronan_cache . Finally, we call generate to produce the answer, decoding the final result with tokenizer.decode . question1 = \"Who is Ronan Takizawa?\" clean_up(ronan_cache, origin_len) input_ids_q1 = tokenizer(question1 + \"\\n\", return_tensors=\"pt\").input_ids.to(device) gen_ids_q1 = generate(model, input_ids_q1, ronan_cache) answer1 = tokenizer.decode(gen_ids_q1[0], skip_special_tokens=True) print(\"Q1:\", question1) print(\"A1:\", answer1) You should expect a response like this: Q1: Who is Ronan Takizawa? A1: Answer: Ronan Takizawa is an ambitious and accomplished tech enthusiast. He has a diverse skill set in software development, AI/ML... Full Code Make sure to leave the ORIGINAL REPO a Star! \u2b50\ufe0f Conclusion Cache-augmented generation (CAG) simplifies AI architectures by storing small knowledge bases directly within a model\u2019s context window, eliminating the need for retrieval loops in RAG and reducing latency. This approach enhances response speed and improves the responsiveness of an LLM with external knowledge. By leveraging CAG, developers can streamline their AI systems for faster and more efficient knowledge integration, particularly for tasks with stable, compact datasets.","title":"Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial"},{"location":"Cache-Augmented%20Generation/#cache-augmented-generation-cag-in-llms-a-step-by-step-tutorial","text":"Retrieval-augmented generation (RAG) is a powerful method to connect external knowledge bases to an LLM and fetch context each time a user asks a question, but it can slow down the LLM\u2019s performance due to its retrieval latency. Cache-augmented generation (CAG) offers a faster alternative; instead of performing real-time retrieval, it preloads your relevant documents into the model\u2019s context and stores that inference state \u2014 also known as a Key-Value (KV) cache. This approach eliminates retrieval latencies, allowing the model to access preloaded information instantly for faster and more efficient responses. For a more technical explanation of CAG, check out this article . In this tutorial, we will show how to build a simple CAG setup to embed all your knowledge upfront, quickly answer multiple user queries, and reset the cache without reloading the entire context each time.","title":"Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial"},{"location":"Cache-Augmented%20Generation/#prerequisites","text":"1. A HuggingFace account and a HuggingFace access token 2. A document.txt file with sentences about yourself.","title":"Prerequisites"},{"location":"Cache-Augmented%20Generation/#project-setup","text":"We import the essential libraries: torch for PyTorch. transformers for Hugging Face. DynamicCache for storing the model\u2019s key-value states. import torch from transformers import AutoTokenizer, AutoModelForCausalLM from transformers.cache_utils import DynamicCache import os","title":"Project Setup"},{"location":"Cache-Augmented%20Generation/#generate-function","text":"We\u2019ll next define the generate function. The generate function handles token-by-token generation with the cached knowledge using greedy decoding. Greedy decoding is a simple text generation method where, at each step, the token with the highest probability (maximum value in the logits) is selected as the next token. We pass in these inputs: model : The LLM, which with me Mistral-7B for this tutorial. input_ids : A tensor containing the tokenized input sequence. past_key_values : The core component of the CAG. A cache of previously computed attention values is used to speed up inference by avoiding recomputation. max_new_tokens : The maximum number of new tokens to generate. The default is 50. The function operates in a loop that iterates up to max_new_tokens times or terminates early if an end-of-sequence token (if configured) is generated. At each iteration: The model processes the current input tokens along with the cached past_key_values , producing logits for the next token. The logits are analyzed to identify the token with the highest probability using greedy decoding. This new token is appended to the output sequence, and the cache ( past_key_values ) is updated to include the current context. The newly generated token becomes the input for the next iteration. def generate(model, input_ids: torch.Tensor, past_key_values, max_new_tokens: int = 50) -> torch.Tensor: device = model.model.embed_tokens.weight.device origin_len = input_ids.shape[-1] input_ids = input_ids.to(device) output_ids = input_ids.clone() next_token = input_ids with torch.no\\_grad(): for \\_ in range(max\\_new\\_tokens): out = model( input\\_ids=next\\_token, past\\_key\\_values=past\\_key\\_values, use\\_cache=True ) logits = out.logits\\[:, -1, :\\] token = torch.argmax(logits, dim=-1, keepdim=True) output\\_ids = torch.cat(\\[output\\_ids, token\\], dim=-1) past\\_key\\_values = out.past\\_key\\_values next\\_token = token.to(device) if model.config.eos\\_token\\_id is not None and token.item() == model.config.eos\\_token\\_id: break return output\\_ids\\[:, origin\\_len:\\]","title":"Generate Function"},{"location":"Cache-Augmented%20Generation/#dynamiccache-setup","text":"Next, we\u2019ll define the get_kv_cache function that prepares a reusable key-value cache for a transformer model\u2019s attention mechanism and the clean_up function that cleans the key-value cache by removing unnecessary entries to ensure that you can answer multiple independent questions without \u201cpolluting\u201d the cache. get_kv_cache passes a prompt (in our case, the knowledge from document.txt ) through the model once, creating a KV cache that records all the hidden states from each layer. get_kv_cache passes in these inputs: model : The transformer model used for encoding the prompt. tokenizer : Tokenizer to convert the prompt into token IDs. prompt : A string input is used as the prompt. and returns an object of the type DynamicCache. The get_kv_cache function first tokenizes the provided prompt using the tokenizer, converts it into input IDs, and then initializes an DynamicCache object to store key-value pairs, and then performs a forward pass through the model with caching enabled ( use_cache=True ). This populates the cache with the key-value pairs resulting from the model's computation. The clean_up trims a DynamicCache object to match the original sequence length by removing any additional tokens added during processing. For each layer of the cache, it slices both the key and value tensors to retain only the first origin_len tokens along the sequence dimension. def get_kv_cache(model, tokenizer, prompt: str) -> DynamicCache: device = model.model.embed_tokens.weight.device input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) cache = DynamicCache() with torch.no\\_grad(): \\_ = model( input\\_ids=input\\_ids, past\\_key\\_values=cache, use\\_cache=True ) return cache def clean_up(cache: DynamicCache, origin_len: int): for i in range(len(cache.key_cache)): cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :] cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]","title":"DynamicCache Setup"},{"location":"Cache-Augmented%20Generation/#load-llm-mistral","text":"Now we\u2019ll load the Mistral-7B model, and load the tokenizer and model in full precision or half precision (FP16) on GPU if available. Remember to input YOUR_HF_TOKEN with your unique HuggingFace Token. model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"YOUR_HF_TOKEN\", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map=\"auto\", trust_remote_code=True, token=\"YOUR_HF_TOKEN\" ) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model.to(device) print(f\"Loaded {model_name}.\")","title":"Load LLM (Mistral)"},{"location":"Cache-Augmented%20Generation/#create-a-knowledge-prompt-from-documenttxt","text":"Next, we\u2019ll read document.txt , which you can fill with information about yourself. For this tutorial, document.txt contains information about me (Ronan Takizawa). Here we construct a simple system prompt embedding with the doc information and pass it to get_kv_cache to generate the KV cache. with open(\"document.txt\", \"r\", encoding=\"utf-8\") as f: doc_text = f.read() system_prompt = f\"\"\" <|system|> You are an assistant who provides concise factual answers. <|user|> Context: {doc_text} Question: \"\"\".strip() ronan_cache = get_kv_cache(model, tokenizer, system_prompt) origin_len = ronan_cache.key_cache[0].shape[-2] print(\"KV cache built.\")","title":"Create a Knowledge Prompt from document.txt"},{"location":"Cache-Augmented%20Generation/#ask-questions-reusing-the-cache","text":"We first run clean_up to clear our cache (Good practice for CAGs). Next, we convert our questions into tokens in input_ids_q1 , then appended to the knowledge context stored in ronan_cache . Finally, we call generate to produce the answer, decoding the final result with tokenizer.decode . question1 = \"Who is Ronan Takizawa?\" clean_up(ronan_cache, origin_len) input_ids_q1 = tokenizer(question1 + \"\\n\", return_tensors=\"pt\").input_ids.to(device) gen_ids_q1 = generate(model, input_ids_q1, ronan_cache) answer1 = tokenizer.decode(gen_ids_q1[0], skip_special_tokens=True) print(\"Q1:\", question1) print(\"A1:\", answer1) You should expect a response like this: Q1: Who is Ronan Takizawa? A1: Answer: Ronan Takizawa is an ambitious and accomplished tech enthusiast. He has a diverse skill set in software development, AI/ML... Full Code","title":"Ask Questions Reusing the Cache"},{"location":"Cache-Augmented%20Generation/#make-sure-to-leave-the-original-repo-a-star","text":"","title":"Make sure to leave the ORIGINAL REPO a Star! \u2b50\ufe0f"},{"location":"Cache-Augmented%20Generation/#conclusion","text":"Cache-augmented generation (CAG) simplifies AI architectures by storing small knowledge bases directly within a model\u2019s context window, eliminating the need for retrieval loops in RAG and reducing latency. This approach enhances response speed and improves the responsiveness of an LLM with external knowledge. By leveraging CAG, developers can streamline their AI systems for faster and more efficient knowledge integration, particularly for tasks with stable, compact datasets.","title":"Conclusion"},{"location":"Chapter_1.1/","text":"Rename the File to Use the .md Extension Navigate to the /docs folder where the file is located. Rename the file to remove the .txt extension, keeping only .md: mv docs/Chapter_1.md.txt docs/Chapter_1.md","title":"Rename the File to Use the .md Extension"},{"location":"Chapter_1.1/#rename-the-file-to-use-the-md-extension","text":"","title":"Rename the File to Use the .md Extension"},{"location":"Chapter_1.1/#navigate-to-the-docs-folder-where-the-file-is-located","text":"Rename the file to remove the .txt extension, keeping only .md: mv docs/Chapter_1.md.txt docs/Chapter_1.md","title":"Navigate to the /docs folder where the file is located."},{"location":"Chapter_1/","text":"Navigate to the Docs Folder: In your project, there will be a folder named docs. This is where all your documentation files (usually in Markdown format) are stored. Open the docs folder in VS Code . # Python Program to find the area of triangle a = 5 b = 6 c = 7 # Uncomment below to take inputs from the user # a = float(input('Enter first side: ')) # b = float(input('Enter second side: ')) # c = float(input('Enter third side: ')) # calculate the semi-perimeter s = (a + b + c) / 2 # calculate the area area = (s*(s-a)*(s-b)*(s-c)) ** 0.5 print('The area of the triangle is %0.2f' %area)","title":"Navigate to the Docs Folder:"},{"location":"Chapter_1/#navigate-to-the-docs-folder","text":"","title":"Navigate to the Docs Folder:"},{"location":"Chapter_1/#in-your-project-there-will-be-a-folder-named-docs-this-is-where-all-your-documentation-files-usually-in-markdown-format-are-stored","text":"Open the docs folder in VS Code . # Python Program to find the area of triangle a = 5 b = 6 c = 7 # Uncomment below to take inputs from the user # a = float(input('Enter first side: ')) # b = float(input('Enter second side: ')) # c = float(input('Enter third side: ')) # calculate the semi-perimeter s = (a + b + c) / 2 # calculate the area area = (s*(s-a)*(s-b)*(s-c)) ** 0.5 print('The area of the triangle is %0.2f' %area)","title":"In your project, there will be a folder named docs. This is where all your documentation files (usually in Markdown format) are stored."}]}